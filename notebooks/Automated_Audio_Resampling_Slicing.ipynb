{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Import and Install Required Libraries\n",
        "\n",
        "In this step, we install and import all the necessary Python libraries\n",
        "(torch, torchaudio, pydub, etc.) that will be used for audio processing,\n",
        "resampling, and voice activity detection."
      ],
      "metadata": {
        "id": "ETDlkderdbs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîó Mount Google Drive\n",
        "\n",
        "Mount Google Drive into the Colab environment so that audio data and processed files can be accessed and saved directly to your Drive."
      ],
      "metadata": {
        "id": "HM8KkoJCdRhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bLKR-fmtAZu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Naija-Language-Accent_ID\")"
      ],
      "metadata": {
        "id": "SgpadHXwtist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Install Required Packages\n",
        "\n",
        "Install additional libraries needed for audio processing and voice activity detection:  \n",
        "- **silero-vad** ‚Üí for Voice Activity Detection (speech/silence segmentation)  \n",
        "- **pydub** ‚Üí for audio slicing, manipulation, and exporting  \n",
        "- **soundfile** ‚Üí for reading and streaming audio files efficiently"
      ],
      "metadata": {
        "id": "gOu9u1-Me8u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q silero-vad pydub soundfile"
      ],
      "metadata": {
        "id": "HIG9E-Zit8Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as tt\n",
        "from pydub import AudioSegment\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VXxQnAVhudWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÇ Define Source and Destination Paths"
      ],
      "metadata": {
        "id": "5-qCqkJmfScm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÇ Source Paths for Raw Audio Data\n",
        "\n",
        "Here we set up the source directories in Google Drive that contain the raw `.wav` audio files for each language (Igbo, Hausa, Yoruba).  \n",
        "\n",
        "A helper function `define_source_paths` is used to iterate through each directory and generate a list of file paths, which will be used later for preprocessing and slicing."
      ],
      "metadata": {
        "id": "eb7P7LorfZMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Google Drive Source Path\n",
        "source_dir_igbo = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/raw_data/spoken_igbo\")\n",
        "source_dir_hausa = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/raw_data/spoken_hausa\")\n",
        "source_dir_yoruba = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/raw_data/spoken_yoruba\")\n",
        "def define_source_paths(source):\n",
        "  list_of_sources = []\n",
        "  for item in source.iterdir():\n",
        "    list_of_sources.append(item)\n",
        "  return list_of_sources\n",
        "\n",
        "igbo_source_path = define_source_paths(source_dir_igbo)\n",
        "hausa_source_path = define_source_paths(source_dir_hausa)\n",
        "yoruba_source_path = define_source_paths(source_dir_yoruba)"
      ],
      "metadata": {
        "id": "XilUN3X7UsNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÇ Destination Paths for Processed Audio Data\n",
        "\n",
        "We now specify the Google Drive directories where the processed (sliced and resampled) audio files will be saved for each language (Igbo, Hausa, Yoruba).  \n",
        "\n",
        "Using the same `define_source_paths` helper, we generate lists of destination paths to organize the output chunks into their respective folders."
      ],
      "metadata": {
        "id": "m1eFSnyLfuu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Google Drive Destination Path\n",
        "processed_dir_igbo = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/processed_audio/spoken_igbo\")\n",
        "processed_dir_hausa = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/processed_audio/spoken_hausa\")\n",
        "processed_dir_yoruba = Path(\"/content/drive/MyDrive/Naija-Language-Accent_ID/Data/processed_audio/spoken_yoruba\")\n",
        "\n",
        "igbo_processed_path = define_source_paths(processed_dir_igbo)\n",
        "hausa_processed_path = define_source_paths(processed_dir_hausa)\n",
        "yoruba_processed_path = define_source_paths(processed_dir_yoruba)"
      ],
      "metadata": {
        "id": "6t0JL8euYWah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üé§ Load Silero VAD Model and Utilities\n",
        "\n",
        "We load the **Silero Voice Activity Detection (VAD)** model directly from\n",
        "[torch hub](https://pytorch.org/hub/), along with its utility functions.  \n",
        "\n",
        "- The `model` is the pretrained PyTorch VAD model.  \n",
        "- The `silero_utils` include helper functions such as `get_speech_timestamps`, which will be used to detect and extract speech segments from audio files.  "
      ],
      "metadata": {
        "id": "6o-jkniUgDSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Silero VAD model üîä and its utility functions from torch hub\n",
        "print(\"Loading VAD model....\")\n",
        "model, silero_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
        "                              model='silero_vad',\n",
        "                              force_reload=True)\n",
        "\n",
        "(get_speech_timestamps, _, _, _, _) = silero_utils"
      ],
      "metadata": {
        "id": "6Xsn56evydVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Define Audio Processing Function\n",
        "\n",
        "We create a reusable function `resample_and_processing` that performs the following steps:\n",
        "\n",
        "1. **Read and Stream Audio**  \n",
        "   - Opens each audio file in chunks (30 seconds at a time) to avoid memory overload.  \n",
        "   - Converts stereo audio to mono if necessary.  \n",
        "\n",
        "2. **Resample Audio**  \n",
        "   - Resamples from the original sample rate to the `16 kHz` target rate required by Silero VAD.  \n",
        "\n",
        "3. **Apply Voice Activity Detection (VAD)**  \n",
        "   - Uses `get_speech_timestamps` to detect regions of speech within the audio.  \n",
        "   - Adjusts timestamps so they remain aligned with the full audio file.  \n",
        "\n",
        "4. **Slice Audio with PyDub**  \n",
        "   - Loads the full original `.wav` file.  \n",
        "   - Uses the speech timestamps to slice out individual speech segments.  \n",
        "   - Resamples each sliced segment to `16 kHz` before saving.  \n",
        "\n",
        "5. **Save Processed Segments**  \n",
        "   - Exports all chunks as `.wav` files into the corresponding processed folder for each audio source.  \n",
        "\n",
        "‚úÖ This function makes it easy to process multiple large audio files automatically and store clean, uniformly formatted speech chunks for training."
      ],
      "metadata": {
        "id": "ljzwn7aYgktV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: Process and Resample Audio in Chunks from Disk\n",
        "\n",
        "def resample_and_processing(source_dir, silero_model, timestamp_func, processed_dir):\n",
        "  \"\"\"\n",
        "  This function takes in a list of paths to audio files,\n",
        "  a silero_vad model and utils and a list of paths to save the processed audio files.\n",
        "  \"\"\"\n",
        "\n",
        "  for index, source_path in enumerate(source_dir):\n",
        "    TARGET_RATE = 16000 # The sample rate Silero VAD expects\n",
        "    print(f\"Processing and resampling audio in chunks from: {source_path}\")\n",
        "    file_name = source_path.name\n",
        "\n",
        "    all_speech_timestamps = []\n",
        "    current_sample_offset = 0\n",
        "    chunk_size_seconds = 30\n",
        "\n",
        "    try:\n",
        "      with sf.SoundFile(source_path, 'r') as audio_file:\n",
        "        ORIGINAL_RATE = audio_file.samplerate\n",
        "\n",
        "        # Resampler object\n",
        "        resampler = tt.Resample(ORIGINAL_RATE, TARGET_RATE, dtype=torch.float32)\n",
        "\n",
        "        chunk_size_sample = chunk_size_seconds * ORIGINAL_RATE  # Chunk size based on original rate\n",
        "\n",
        "        for block in audio_file.blocks(blocksize=chunk_size_sample, dtype='float32', fill_value=0):\n",
        "          # If the audio is stereo, convert to mono by averaging channels\n",
        "          if block.ndim > 1:\n",
        "            block = np.mean(block, axis=1)\n",
        "\n",
        "          # The 'block' is a numpy array. Convert to a tensor for the model.\n",
        "          audio_tensor_chunk = torch.from_numpy(block)\n",
        "\n",
        "          # Resample the chunk\n",
        "          resampled_chunk = resampler(audio_tensor_chunk)\n",
        "\n",
        "          # Get timestamps using the RESAMPLED chunk and TARGET rate\n",
        "          speech_timestamps = timestamp_func(resampled_chunk, silero_model, sampling_rate=TARGET_RATE)\n",
        "\n",
        "          # Adjust timestamps relative to the RESAMPLED audio timeline\n",
        "          for ts in speech_timestamps:\n",
        "              ts['start'] += current_sample_offset\n",
        "              ts['end'] += current_sample_offset\n",
        "\n",
        "          all_speech_timestamps.extend(speech_timestamps)\n",
        "\n",
        "          # Update the offset based on the length of the RESAMPLED chunk\n",
        "          current_sample_offset += len(resampled_chunk)\n",
        "\n",
        "      print(f\"‚úÖ Found {len(all_speech_timestamps)} speech segments in {source_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing audio file: {e}\")\n",
        "      exit()\n",
        "\n",
        "    # Slicing with pydub\n",
        "    print(\"Slicing audio file with pydub...\")\n",
        "    original_audio = AudioSegment.from_wav(source_path)\n",
        "\n",
        "    # When slicing, resample the final output to 16kHz\n",
        "    # Pydub slices the original file. Set the frame rate of the exported chunk.\n",
        "    for i, ts in enumerate(all_speech_timestamps):\n",
        "      # Timestamps are based on the 16kHz timeline, so we convert them to milliseconds\n",
        "      start_ms = int((ts['start'] / TARGET_RATE) * 1000)\n",
        "      end_ms = int((ts['end'] / TARGET_RATE) * 1000)\n",
        "\n",
        "      speech_segment = original_audio[start_ms:end_ms]\n",
        "\n",
        "      # Resample the final sliced segment before saving\n",
        "      # This ensures the saved chunk is also 16kHz\n",
        "      speech_segment = speech_segment.set_frame_rate(TARGET_RATE)\n",
        "\n",
        "      output_filename = f\"{Path(file_name).stem}_chunk_{i}.wav\"\n",
        "      output_path = processed_dir[index] / output_filename\n",
        "\n",
        "      speech_segment.export(output_path, format=\"wav\")\n",
        "\n",
        "    print(f\"‚úÖ Slicing complete! {len(all_speech_timestamps)} files saved to {processed_dir[index]}\")\n",
        "\n",
        "    all_speech_timestamps = []\n",
        "\n",
        "  print(f\"All resampling and preprocessing has been completed and exported to respective folders\")"
      ],
      "metadata": {
        "id": "4cRemP5kzV9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ñ∂Ô∏è Run Preprocessing on Igbo Dataset\n",
        "\n",
        "We now call the `resample_and_processing` function on the **Igbo audio dataset**"
      ],
      "metadata": {
        "id": "sU_h66DehOxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_and_processing(source_dir=igbo_source_path,\n",
        "                        silero_model=model,\n",
        "                        timestamp_func=get_speech_timestamps,\n",
        "                        processed_dir=igbo_processed_path)"
      ],
      "metadata": {
        "id": "f_6Ki2-veVrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ñ∂Ô∏è Run Preprocessing on Hausa Dataset\n",
        "\n",
        "We now call the `resample_and_processing` function on the **Hausa audio dataset**"
      ],
      "metadata": {
        "id": "Q_4YvUHChWLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_and_processing(source_dir=hausa_source_path,\n",
        "                        silero_model=model,\n",
        "                        timestamp_func=get_speech_timestamps,\n",
        "                        processed_dir=hausa_processed_path)"
      ],
      "metadata": {
        "id": "YHTDhWJveUhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ñ∂Ô∏è Run Preprocessing on Yoruba Dataset\n",
        "\n",
        "We now call the `resample_and_processing` function on the **Yoruba audio dataset**"
      ],
      "metadata": {
        "id": "5utBsZeahdRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resample_and_processing(source_dir=yoruba_source_path,\n",
        "                        silero_model=model,\n",
        "                        timestamp_func=get_speech_timestamps,\n",
        "                        processed_dir=yoruba_processed_path)"
      ],
      "metadata": {
        "id": "ef0vUvh_BdSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNZoaZ5PDUBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}